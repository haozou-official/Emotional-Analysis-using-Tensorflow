{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imdb电影评论情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "from random import randint\n",
    "from os.path import isfile, join\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "载入word列表: (400000,) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "wordsList = np.load('/Users/zouhao/Desktop/LSTM情感分析Test4/wordsList.npy')\n",
    "print('载入word列表:',np.shape(wordsList),type(wordsList))\n",
    "# 这里转化为列表,是二进制编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsList = wordsList.tolist()\n",
    "# 转化为utf-8编码的形式\n",
    "wordsList = [word.decode('UTF-8')  for word in wordsList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " ',',\n",
       " '.',\n",
       " 'of',\n",
       " 'to',\n",
       " 'and',\n",
       " 'in',\n",
       " 'a',\n",
       " '\"',\n",
       " \"'s\",\n",
       " 'for',\n",
       " '-',\n",
       " 'that',\n",
       " 'on',\n",
       " 'is',\n",
       " 'was',\n",
       " 'said',\n",
       " 'with',\n",
       " 'he',\n",
       " 'as',\n",
       " 'it',\n",
       " 'by',\n",
       " 'at',\n",
       " '(',\n",
       " ')',\n",
       " 'from',\n",
       " 'his',\n",
       " \"''\",\n",
       " '``',\n",
       " 'an',\n",
       " 'be',\n",
       " 'has',\n",
       " 'are',\n",
       " 'have',\n",
       " 'but',\n",
       " 'were',\n",
       " 'not',\n",
       " 'this',\n",
       " 'who',\n",
       " 'they',\n",
       " 'had',\n",
       " 'i',\n",
       " 'which',\n",
       " 'will',\n",
       " 'their',\n",
       " ':',\n",
       " 'or',\n",
       " 'its',\n",
       " 'one',\n",
       " 'after',\n",
       " 'new',\n",
       " 'been',\n",
       " 'also',\n",
       " 'we',\n",
       " 'would',\n",
       " 'two',\n",
       " 'more',\n",
       " \"'\",\n",
       " 'first',\n",
       " 'about',\n",
       " 'up',\n",
       " 'when',\n",
       " 'year',\n",
       " 'there',\n",
       " 'all',\n",
       " '--',\n",
       " 'out',\n",
       " 'she',\n",
       " 'other',\n",
       " 'people',\n",
       " \"n't\",\n",
       " 'her',\n",
       " 'percent',\n",
       " 'than',\n",
       " 'over',\n",
       " 'into',\n",
       " 'last',\n",
       " 'some',\n",
       " 'government',\n",
       " 'time',\n",
       " '$',\n",
       " 'you',\n",
       " 'years',\n",
       " 'if',\n",
       " 'no',\n",
       " 'world',\n",
       " 'can',\n",
       " 'three',\n",
       " 'do',\n",
       " ';',\n",
       " 'president',\n",
       " 'only',\n",
       " 'state',\n",
       " 'million',\n",
       " 'could',\n",
       " 'us',\n",
       " 'most',\n",
       " '_',\n",
       " 'against',\n",
       " 'u.s.',\n",
       " 'so',\n",
       " 'them',\n",
       " 'what',\n",
       " 'him',\n",
       " 'united',\n",
       " 'during',\n",
       " 'before',\n",
       " 'may',\n",
       " 'since',\n",
       " 'many',\n",
       " 'while',\n",
       " 'where',\n",
       " 'states',\n",
       " 'because',\n",
       " 'now',\n",
       " 'city',\n",
       " 'made',\n",
       " 'like',\n",
       " 'between',\n",
       " 'did',\n",
       " 'just',\n",
       " 'national',\n",
       " 'day',\n",
       " 'country',\n",
       " 'under',\n",
       " 'such',\n",
       " 'second',\n",
       " 'then',\n",
       " 'company',\n",
       " 'group',\n",
       " 'any',\n",
       " 'through',\n",
       " 'china',\n",
       " 'four',\n",
       " 'being',\n",
       " 'down',\n",
       " 'war',\n",
       " 'back',\n",
       " 'off',\n",
       " 'south',\n",
       " 'american',\n",
       " 'minister',\n",
       " 'police',\n",
       " 'well',\n",
       " 'including',\n",
       " 'team',\n",
       " 'international',\n",
       " 'week',\n",
       " 'officials',\n",
       " 'still',\n",
       " 'both',\n",
       " 'even',\n",
       " 'high',\n",
       " 'part',\n",
       " 'told',\n",
       " 'those',\n",
       " 'end',\n",
       " 'former',\n",
       " 'these',\n",
       " 'make',\n",
       " 'billion',\n",
       " 'work',\n",
       " 'our',\n",
       " 'home',\n",
       " 'school',\n",
       " 'party',\n",
       " 'house',\n",
       " 'old',\n",
       " 'later',\n",
       " 'get',\n",
       " 'another',\n",
       " 'tuesday',\n",
       " 'news',\n",
       " 'long',\n",
       " 'five',\n",
       " 'called',\n",
       " '1',\n",
       " 'wednesday',\n",
       " 'military',\n",
       " 'way',\n",
       " 'used',\n",
       " 'much',\n",
       " 'next',\n",
       " 'monday',\n",
       " 'thursday',\n",
       " 'friday',\n",
       " 'game',\n",
       " 'here',\n",
       " '?',\n",
       " 'should',\n",
       " 'take',\n",
       " 'very',\n",
       " 'my',\n",
       " 'north',\n",
       " 'security',\n",
       " 'season',\n",
       " 'york',\n",
       " 'how',\n",
       " 'public',\n",
       " 'early',\n",
       " 'according',\n",
       " 'several',\n",
       " 'court',\n",
       " 'say',\n",
       " 'around',\n",
       " 'foreign',\n",
       " '10',\n",
       " 'until',\n",
       " 'set',\n",
       " 'political',\n",
       " 'says',\n",
       " 'market',\n",
       " 'however',\n",
       " 'family',\n",
       " 'life',\n",
       " 'same',\n",
       " 'general',\n",
       " '–',\n",
       " 'left',\n",
       " 'good',\n",
       " 'top',\n",
       " 'university',\n",
       " 'going',\n",
       " 'number',\n",
       " 'major',\n",
       " 'known',\n",
       " 'points',\n",
       " 'won',\n",
       " 'six',\n",
       " 'month',\n",
       " 'dollars',\n",
       " 'bank',\n",
       " '2',\n",
       " 'iraq',\n",
       " 'use',\n",
       " 'members',\n",
       " 'each',\n",
       " 'area',\n",
       " 'found',\n",
       " 'official',\n",
       " 'sunday',\n",
       " 'place',\n",
       " 'go',\n",
       " 'based',\n",
       " 'among',\n",
       " 'third',\n",
       " 'times',\n",
       " 'took',\n",
       " 'right',\n",
       " 'days',\n",
       " 'local',\n",
       " 'economic',\n",
       " 'countries',\n",
       " 'see',\n",
       " 'best',\n",
       " 'report',\n",
       " 'killed',\n",
       " 'held',\n",
       " 'business',\n",
       " 'west',\n",
       " 'does',\n",
       " 'own',\n",
       " '%',\n",
       " 'came',\n",
       " 'law',\n",
       " 'months',\n",
       " 'women',\n",
       " \"'re\",\n",
       " 'power',\n",
       " 'think',\n",
       " 'service',\n",
       " 'children',\n",
       " 'bush',\n",
       " 'show',\n",
       " '/',\n",
       " 'help',\n",
       " 'chief',\n",
       " 'saturday',\n",
       " 'system',\n",
       " 'john',\n",
       " 'support',\n",
       " 'series',\n",
       " 'play',\n",
       " 'office',\n",
       " 'following',\n",
       " 'me',\n",
       " 'meeting',\n",
       " 'expected',\n",
       " 'late',\n",
       " 'washington',\n",
       " 'games',\n",
       " 'european',\n",
       " 'league',\n",
       " 'reported',\n",
       " 'final',\n",
       " 'added',\n",
       " 'without',\n",
       " 'british',\n",
       " 'white',\n",
       " 'history',\n",
       " 'man',\n",
       " 'men',\n",
       " 'became',\n",
       " 'want',\n",
       " 'march',\n",
       " 'case',\n",
       " 'few',\n",
       " 'run',\n",
       " 'money',\n",
       " 'began',\n",
       " 'open',\n",
       " 'name',\n",
       " 'trade',\n",
       " 'center',\n",
       " '3',\n",
       " 'israel',\n",
       " 'oil',\n",
       " 'too',\n",
       " 'al',\n",
       " 'film',\n",
       " 'win',\n",
       " 'led',\n",
       " 'east',\n",
       " 'central',\n",
       " '20',\n",
       " 'air',\n",
       " 'come',\n",
       " 'chinese',\n",
       " 'town',\n",
       " 'leader',\n",
       " 'army',\n",
       " 'line',\n",
       " 'never',\n",
       " 'little',\n",
       " 'played',\n",
       " 'prime',\n",
       " 'death',\n",
       " 'companies',\n",
       " 'least',\n",
       " 'put',\n",
       " 'forces',\n",
       " 'past',\n",
       " 'de',\n",
       " 'half',\n",
       " 'june',\n",
       " 'saying',\n",
       " 'know',\n",
       " 'federal',\n",
       " 'french',\n",
       " 'peace',\n",
       " 'earlier',\n",
       " 'capital',\n",
       " 'force',\n",
       " 'great',\n",
       " 'union',\n",
       " 'near',\n",
       " 'released',\n",
       " 'small',\n",
       " 'department',\n",
       " 'every',\n",
       " 'health',\n",
       " 'japan',\n",
       " 'head',\n",
       " 'ago',\n",
       " 'night',\n",
       " 'big',\n",
       " 'cup',\n",
       " 'election',\n",
       " 'region',\n",
       " 'director',\n",
       " 'talks',\n",
       " 'program',\n",
       " 'far',\n",
       " 'today',\n",
       " 'statement',\n",
       " 'july',\n",
       " 'although',\n",
       " 'district',\n",
       " 'again',\n",
       " 'born',\n",
       " 'development',\n",
       " 'leaders',\n",
       " 'council',\n",
       " 'close',\n",
       " 'record',\n",
       " 'along',\n",
       " 'county',\n",
       " 'france',\n",
       " 'went',\n",
       " 'point',\n",
       " 'must',\n",
       " 'spokesman',\n",
       " 'your',\n",
       " 'member',\n",
       " 'plan',\n",
       " 'financial',\n",
       " 'april',\n",
       " 'recent',\n",
       " 'campaign',\n",
       " 'become',\n",
       " 'troops',\n",
       " 'whether',\n",
       " 'lost',\n",
       " 'music',\n",
       " '15',\n",
       " 'got',\n",
       " 'israeli',\n",
       " '30',\n",
       " 'need',\n",
       " '4',\n",
       " 'lead',\n",
       " 'already',\n",
       " 'russia',\n",
       " 'though',\n",
       " 'might',\n",
       " 'free',\n",
       " 'hit',\n",
       " 'rights',\n",
       " '11',\n",
       " 'information',\n",
       " 'away',\n",
       " '12',\n",
       " '5',\n",
       " 'others',\n",
       " 'control',\n",
       " 'within',\n",
       " 'large',\n",
       " 'economy',\n",
       " 'press',\n",
       " 'agency',\n",
       " 'water',\n",
       " 'died',\n",
       " 'career',\n",
       " 'making',\n",
       " '...',\n",
       " 'deal',\n",
       " 'attack',\n",
       " 'side',\n",
       " 'seven',\n",
       " 'better',\n",
       " 'less',\n",
       " 'september',\n",
       " 'once',\n",
       " 'clinton',\n",
       " 'main',\n",
       " 'due',\n",
       " 'committee',\n",
       " 'building',\n",
       " 'conference',\n",
       " 'club',\n",
       " 'january',\n",
       " 'decision',\n",
       " 'stock',\n",
       " 'america',\n",
       " 'given',\n",
       " 'give',\n",
       " 'often',\n",
       " 'announced',\n",
       " 'television',\n",
       " 'industry',\n",
       " 'order',\n",
       " 'young',\n",
       " \"'ve\",\n",
       " 'palestinian',\n",
       " 'age',\n",
       " 'start',\n",
       " 'administration',\n",
       " 'russian',\n",
       " 'prices',\n",
       " 'round',\n",
       " 'december',\n",
       " 'nations',\n",
       " \"'m\",\n",
       " 'human',\n",
       " 'india',\n",
       " 'defense',\n",
       " 'asked',\n",
       " 'total',\n",
       " 'october',\n",
       " 'players',\n",
       " 'bill',\n",
       " 'important',\n",
       " 'southern',\n",
       " 'move',\n",
       " 'fire',\n",
       " 'population',\n",
       " 'rose',\n",
       " 'november',\n",
       " 'include',\n",
       " 'further',\n",
       " 'nuclear',\n",
       " 'street',\n",
       " 'taken',\n",
       " 'media',\n",
       " 'different',\n",
       " 'issue',\n",
       " 'received',\n",
       " 'secretary',\n",
       " 'return',\n",
       " 'college',\n",
       " 'working',\n",
       " 'community',\n",
       " 'eight',\n",
       " 'groups',\n",
       " 'despite',\n",
       " 'level',\n",
       " 'largest',\n",
       " 'whose',\n",
       " 'attacks',\n",
       " 'germany',\n",
       " 'august',\n",
       " 'change',\n",
       " 'church',\n",
       " 'nation',\n",
       " 'german',\n",
       " 'station',\n",
       " 'london',\n",
       " 'weeks',\n",
       " 'having',\n",
       " '18',\n",
       " 'research',\n",
       " 'black',\n",
       " 'services',\n",
       " 'story',\n",
       " '6',\n",
       " 'europe',\n",
       " 'sales',\n",
       " 'policy',\n",
       " 'visit',\n",
       " 'northern',\n",
       " 'lot',\n",
       " 'across',\n",
       " 'per',\n",
       " 'current',\n",
       " 'board',\n",
       " 'football',\n",
       " 'ministry',\n",
       " 'workers',\n",
       " 'vote',\n",
       " 'book',\n",
       " 'fell',\n",
       " 'seen',\n",
       " 'role',\n",
       " 'students',\n",
       " 'shares',\n",
       " 'iran',\n",
       " 'process',\n",
       " 'agreement',\n",
       " 'quarter',\n",
       " 'full',\n",
       " 'match',\n",
       " 'started',\n",
       " 'growth',\n",
       " 'yet',\n",
       " 'moved',\n",
       " 'possible',\n",
       " 'western',\n",
       " 'special',\n",
       " '100',\n",
       " 'plans',\n",
       " 'interest',\n",
       " 'behind',\n",
       " 'strong',\n",
       " 'england',\n",
       " 'named',\n",
       " 'food',\n",
       " 'period',\n",
       " 'real',\n",
       " 'authorities',\n",
       " 'car',\n",
       " 'term',\n",
       " 'rate',\n",
       " 'race',\n",
       " 'nearly',\n",
       " 'korea',\n",
       " 'enough',\n",
       " 'site',\n",
       " 'opposition',\n",
       " 'keep',\n",
       " '25',\n",
       " 'call',\n",
       " 'future',\n",
       " 'taking',\n",
       " 'island',\n",
       " '2008',\n",
       " '2006',\n",
       " 'road',\n",
       " 'outside',\n",
       " 'really',\n",
       " 'century',\n",
       " 'democratic',\n",
       " 'almost',\n",
       " 'single',\n",
       " 'share',\n",
       " 'leading',\n",
       " 'trying',\n",
       " 'find',\n",
       " 'album',\n",
       " 'senior',\n",
       " 'minutes',\n",
       " 'together',\n",
       " 'congress',\n",
       " 'index',\n",
       " 'australia',\n",
       " 'results',\n",
       " 'hard',\n",
       " 'hours',\n",
       " 'land',\n",
       " 'action',\n",
       " 'higher',\n",
       " 'field',\n",
       " 'cut',\n",
       " 'coach',\n",
       " 'elections',\n",
       " 'san',\n",
       " 'issues',\n",
       " 'executive',\n",
       " 'february',\n",
       " 'production',\n",
       " 'areas',\n",
       " 'river',\n",
       " 'face',\n",
       " 'using',\n",
       " 'japanese',\n",
       " 'province',\n",
       " 'park',\n",
       " 'price',\n",
       " 'commission',\n",
       " 'california',\n",
       " 'father',\n",
       " 'son',\n",
       " 'education',\n",
       " '7',\n",
       " 'village',\n",
       " 'energy',\n",
       " 'shot',\n",
       " 'short',\n",
       " 'africa',\n",
       " 'key',\n",
       " 'red',\n",
       " 'association',\n",
       " 'average',\n",
       " 'pay',\n",
       " 'exchange',\n",
       " 'eu',\n",
       " 'something',\n",
       " 'gave',\n",
       " 'likely',\n",
       " 'player',\n",
       " 'george',\n",
       " '2007',\n",
       " 'victory',\n",
       " '8',\n",
       " 'low',\n",
       " 'things',\n",
       " '2010',\n",
       " 'pakistan',\n",
       " '14',\n",
       " 'post',\n",
       " 'social',\n",
       " 'continue',\n",
       " 'ever',\n",
       " 'look',\n",
       " 'chairman',\n",
       " 'job',\n",
       " '2000',\n",
       " 'soldiers',\n",
       " 'able',\n",
       " 'parliament',\n",
       " 'front',\n",
       " 'himself',\n",
       " 'problems',\n",
       " 'private',\n",
       " 'lower',\n",
       " 'list',\n",
       " 'built',\n",
       " '13',\n",
       " 'efforts',\n",
       " 'dollar',\n",
       " 'miles',\n",
       " 'included',\n",
       " 'radio',\n",
       " 'live',\n",
       " 'form',\n",
       " 'david',\n",
       " 'african',\n",
       " 'increase',\n",
       " 'reports',\n",
       " 'sent',\n",
       " 'fourth',\n",
       " 'always',\n",
       " 'king',\n",
       " '50',\n",
       " 'tax',\n",
       " 'taiwan',\n",
       " 'britain',\n",
       " '16',\n",
       " 'playing',\n",
       " 'title',\n",
       " 'middle',\n",
       " 'meet',\n",
       " 'global',\n",
       " 'wife',\n",
       " '2009',\n",
       " 'position',\n",
       " 'located',\n",
       " 'clear',\n",
       " 'ahead',\n",
       " '2004',\n",
       " '2005',\n",
       " 'iraqi',\n",
       " 'english',\n",
       " 'result',\n",
       " 'release',\n",
       " 'violence',\n",
       " 'goal',\n",
       " 'project',\n",
       " 'closed',\n",
       " 'border',\n",
       " 'body',\n",
       " 'soon',\n",
       " 'crisis',\n",
       " 'division',\n",
       " '&amp;',\n",
       " 'served',\n",
       " 'tour',\n",
       " 'hospital',\n",
       " 'kong',\n",
       " 'test',\n",
       " 'hong',\n",
       " 'u.n.',\n",
       " 'inc.',\n",
       " 'technology',\n",
       " 'believe',\n",
       " 'organization',\n",
       " 'published',\n",
       " 'weapons',\n",
       " 'agreed',\n",
       " 'why',\n",
       " 'nine',\n",
       " 'summer',\n",
       " 'wanted',\n",
       " 'republican',\n",
       " 'act',\n",
       " 'recently',\n",
       " 'texas',\n",
       " 'course',\n",
       " 'problem',\n",
       " 'senate',\n",
       " 'medical',\n",
       " 'un',\n",
       " 'done',\n",
       " 'reached',\n",
       " 'star',\n",
       " 'continued',\n",
       " 'investors',\n",
       " 'living',\n",
       " 'care',\n",
       " 'signed',\n",
       " '17',\n",
       " 'art',\n",
       " 'provide',\n",
       " 'worked',\n",
       " 'presidential',\n",
       " 'gold',\n",
       " 'obama',\n",
       " 'morning',\n",
       " 'dead',\n",
       " 'opened',\n",
       " \"'ll\",\n",
       " 'event',\n",
       " 'previous',\n",
       " 'cost',\n",
       " 'instead',\n",
       " 'canada',\n",
       " 'band',\n",
       " 'teams',\n",
       " 'daily',\n",
       " '2001',\n",
       " 'available',\n",
       " 'drug',\n",
       " 'coming',\n",
       " '2003',\n",
       " 'investment',\n",
       " '’s',\n",
       " 'michael',\n",
       " 'civil',\n",
       " 'woman',\n",
       " 'training',\n",
       " 'appeared',\n",
       " '9',\n",
       " 'involved',\n",
       " 'indian',\n",
       " 'similar',\n",
       " 'situation',\n",
       " '24',\n",
       " 'los',\n",
       " 'running',\n",
       " 'fighting',\n",
       " 'mark',\n",
       " '40',\n",
       " 'trial',\n",
       " 'hold',\n",
       " 'australian',\n",
       " 'thought',\n",
       " '!',\n",
       " 'study',\n",
       " 'fall',\n",
       " 'mother',\n",
       " 'met',\n",
       " 'relations',\n",
       " 'anti',\n",
       " '2002',\n",
       " 'song',\n",
       " 'popular',\n",
       " 'base',\n",
       " 'tv',\n",
       " 'ground',\n",
       " 'markets',\n",
       " 'ii',\n",
       " 'newspaper',\n",
       " 'staff',\n",
       " 'saw',\n",
       " 'hand',\n",
       " 'hope',\n",
       " 'operations',\n",
       " 'pressure',\n",
       " 'americans',\n",
       " 'eastern',\n",
       " 'st.',\n",
       " 'legal',\n",
       " 'asia',\n",
       " 'budget',\n",
       " 'returned',\n",
       " 'considered',\n",
       " 'love',\n",
       " 'wrote',\n",
       " 'stop',\n",
       " 'fight',\n",
       " 'currently',\n",
       " 'charges',\n",
       " 'try',\n",
       " 'aid',\n",
       " 'ended',\n",
       " 'management',\n",
       " 'brought',\n",
       " 'cases',\n",
       " 'decided',\n",
       " 'failed',\n",
       " 'network',\n",
       " 'works',\n",
       " 'gas',\n",
       " 'turned',\n",
       " 'fact',\n",
       " 'vice',\n",
       " 'ca',\n",
       " 'mexico',\n",
       " 'trading',\n",
       " 'especially',\n",
       " 'reporters',\n",
       " 'afghanistan',\n",
       " 'common',\n",
       " 'looking',\n",
       " 'space',\n",
       " 'rates',\n",
       " 'manager',\n",
       " 'loss',\n",
       " '2011',\n",
       " 'justice',\n",
       " 'thousands',\n",
       " 'james',\n",
       " 'rather',\n",
       " 'fund',\n",
       " 'thing',\n",
       " 'republic',\n",
       " 'opening',\n",
       " 'accused',\n",
       " 'winning',\n",
       " 'scored',\n",
       " 'championship',\n",
       " 'example',\n",
       " 'getting',\n",
       " 'biggest',\n",
       " 'performance',\n",
       " 'sports',\n",
       " '1998',\n",
       " 'let',\n",
       " 'allowed',\n",
       " 'schools',\n",
       " 'means',\n",
       " 'turn',\n",
       " 'leave',\n",
       " 'no.',\n",
       " 'robert',\n",
       " 'personal',\n",
       " 'stocks',\n",
       " 'showed',\n",
       " 'light',\n",
       " 'arrested',\n",
       " 'person',\n",
       " 'either',\n",
       " 'offer',\n",
       " 'majority',\n",
       " 'battle',\n",
       " '19',\n",
       " 'class',\n",
       " 'evidence',\n",
       " 'makes',\n",
       " 'society',\n",
       " 'products',\n",
       " 'regional',\n",
       " 'needed',\n",
       " 'stage',\n",
       " 'am',\n",
       " 'doing',\n",
       " 'families',\n",
       " 'construction',\n",
       " 'various',\n",
       " '1996',\n",
       " 'sold',\n",
       " 'independent',\n",
       " 'kind',\n",
       " 'airport',\n",
       " 'paul',\n",
       " 'judge',\n",
       " 'internet',\n",
       " 'movement',\n",
       " 'room',\n",
       " 'followed',\n",
       " 'original',\n",
       " 'angeles',\n",
       " 'italy',\n",
       " '`',\n",
       " 'data',\n",
       " 'comes',\n",
       " 'parties',\n",
       " 'nothing',\n",
       " 'sea',\n",
       " 'bring',\n",
       " '2012',\n",
       " 'annual',\n",
       " 'officer',\n",
       " 'beijing',\n",
       " 'present',\n",
       " 'remain',\n",
       " 'nato',\n",
       " '1999',\n",
       " '22',\n",
       " 'remains',\n",
       " 'allow',\n",
       " 'florida',\n",
       " 'computer',\n",
       " '21',\n",
       " 'contract',\n",
       " 'coast',\n",
       " 'created',\n",
       " 'demand',\n",
       " 'operation',\n",
       " 'events',\n",
       " 'islamic',\n",
       " 'beat',\n",
       " 'analysts',\n",
       " 'interview',\n",
       " 'helped',\n",
       " 'child',\n",
       " 'probably',\n",
       " 'spent',\n",
       " 'asian',\n",
       " 'effort',\n",
       " 'cooperation',\n",
       " 'shows',\n",
       " 'calls',\n",
       " 'investigation',\n",
       " 'lives',\n",
       " 'video',\n",
       " 'yen',\n",
       " 'runs',\n",
       " 'tried',\n",
       " 'bad',\n",
       " 'described',\n",
       " '1994',\n",
       " 'toward',\n",
       " 'written',\n",
       " 'throughout',\n",
       " 'established',\n",
       " 'mission',\n",
       " 'associated',\n",
       " 'buy',\n",
       " 'growing',\n",
       " 'green',\n",
       " 'forward',\n",
       " 'competition',\n",
       " 'poor',\n",
       " 'latest',\n",
       " 'banks',\n",
       " 'question',\n",
       " '1997',\n",
       " 'prison',\n",
       " 'feel',\n",
       " 'attention',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsList#list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "载入文本向量: (400000, 50)\n"
     ]
    }
   ],
   "source": [
    "# 400000x50的嵌入矩阵，训练好的词典向量模型\n",
    "wordVectors = np.load('/Users/zouhao/Desktop/LSTM情感分析Test4/wordVectors.npy')\n",
    "print('载入文本向量:',wordVectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.013441 ,  0.23682  , -0.16899  , ..., -0.56657  ,  0.044691 ,\n",
       "         0.30392  ],\n",
       "       [ 0.15164  ,  0.30177  , -0.16763  , ..., -0.35652  ,  0.016413 ,\n",
       "         0.10216  ],\n",
       "       ...,\n",
       "       [-0.51181  ,  0.058706 ,  1.0913   , ..., -0.25003  , -1.125    ,\n",
       "         1.5863   ],\n",
       "       [-0.75898  , -0.47426  ,  0.4737   , ...,  0.78954  , -0.014116 ,\n",
       "         0.6448   ],\n",
       "       [-0.79149  ,  0.86617  ,  0.11998  , ..., -0.29996  , -0.0063003,\n",
       "         0.3954   ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.3888e-01,  5.0514e-01, -4.5316e-02, -1.3606e+00,  6.7850e-01,\n",
       "        1.4193e+00, -8.5180e-01, -6.6912e-01,  4.5732e-01, -1.5827e-01,\n",
       "       -3.7624e-02, -2.0835e-01,  5.4524e-01, -6.4056e-01,  1.2267e+00,\n",
       "       -2.6497e-01, -4.5656e-01,  2.7901e-01,  3.8550e-01,  1.4437e-01,\n",
       "        2.6343e-03,  1.7084e+00,  5.6702e-01,  6.0751e-01,  4.3173e-01,\n",
       "       -1.1765e+00, -4.5223e-01, -9.8725e-01,  1.9464e-01, -6.4467e-01,\n",
       "        3.1015e+00,  1.0881e+00, -1.7687e-01, -8.1104e-01,  2.8289e-01,\n",
       "        6.5915e-01,  3.8012e-01, -2.7123e-01,  6.2490e-01, -5.4019e-02,\n",
       "       -6.4090e-01,  1.0671e-01,  7.9348e-01,  1.0989e-01,  6.6735e-01,\n",
       "       -6.5263e-02, -6.3952e-01, -1.2490e+00, -3.4498e-02,  1.8429e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "childrenIndex = wordsList.index('children')\n",
    "wordVectors[childrenIndex]\n",
    "#词向量库的词和列表词的索引是一一对应的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4715_9.txt',\n",
       " '12390_8.txt',\n",
       " '8329_7.txt',\n",
       " '9063_8.txt',\n",
       " '3092_10.txt',\n",
       " '9865_8.txt',\n",
       " '6639_10.txt',\n",
       " '10460_10.txt',\n",
       " '10331_10.txt',\n",
       " '11606_10.txt',\n",
       " '6168_10.txt',\n",
       " '2712_10.txt',\n",
       " '3225_10.txt',\n",
       " '3574_10.txt',\n",
       " '3192_10.txt',\n",
       " '716_10.txt',\n",
       " '2612_10.txt',\n",
       " '5568_8.txt',\n",
       " '6554_7.txt',\n",
       " '1807_7.txt',\n",
       " '3474_10.txt',\n",
       " '11057_10.txt',\n",
       " '10231_10.txt',\n",
       " '11706_10.txt',\n",
       " '11167_9.txt',\n",
       " '803_10.txt',\n",
       " '5245_8.txt',\n",
       " '7935_8.txt',\n",
       " '835_8.txt',\n",
       " '6970_8.txt',\n",
       " '9533_9.txt',\n",
       " '5393_10.txt',\n",
       " '3384_8.txt',\n",
       " '6935_8.txt',\n",
       " '4342_10.txt',\n",
       " '9576_9.txt',\n",
       " '11913_10.txt',\n",
       " '5124_10.txt',\n",
       " '10961_9.txt',\n",
       " '2835_7.txt',\n",
       " '9919_9.txt',\n",
       " '9177_10.txt',\n",
       " '6762_9.txt',\n",
       " '12091_8.txt',\n",
       " '4414_9.txt',\n",
       " '9491_10.txt',\n",
       " '6115_10.txt',\n",
       " '7422_10.txt',\n",
       " '3258_10.txt',\n",
       " '6727_9.txt',\n",
       " '9327_8.txt',\n",
       " '4451_9.txt',\n",
       " '3509_10.txt',\n",
       " '2389_10.txt',\n",
       " '1936_10.txt',\n",
       " '6284_7.txt',\n",
       " '7727_9.txt',\n",
       " '1794_7.txt',\n",
       " '2902_9.txt',\n",
       " '3409_10.txt',\n",
       " '9591_10.txt',\n",
       " '2379_8.txt',\n",
       " '6655_7.txt',\n",
       " '58_9.txt',\n",
       " '11773_8.txt',\n",
       " '7232_8.txt',\n",
       " '8632_9.txt',\n",
       " '8898_8.txt',\n",
       " '6178_7.txt',\n",
       " '9740_7.txt',\n",
       " '7849_9.txt',\n",
       " '1623_10.txt',\n",
       " '8677_9.txt',\n",
       " '5708_10.txt',\n",
       " '2629_9.txt',\n",
       " '11223_9.txt',\n",
       " '7980_10.txt',\n",
       " '12480_10.txt',\n",
       " '10266_9.txt',\n",
       " '3454_8.txt',\n",
       " '4907_8.txt',\n",
       " '1988_9.txt',\n",
       " '6951_10.txt',\n",
       " '12166_10.txt',\n",
       " '5159_10.txt',\n",
       " '10858_8.txt',\n",
       " '9677_9.txt',\n",
       " '1767_8.txt',\n",
       " '4101_8.txt',\n",
       " '1072_10.txt',\n",
       " '2221_8.txt',\n",
       " '7447_8.txt',\n",
       " '8047_9.txt',\n",
       " '3928_7.txt',\n",
       " '1640_10.txt',\n",
       " '2919_10.txt',\n",
       " '1111_10.txt',\n",
       " '11881_9.txt',\n",
       " '12005_10.txt',\n",
       " '6832_10.txt',\n",
       " '9976_7.txt',\n",
       " '7402_8.txt',\n",
       " '5774_8.txt',\n",
       " '6399_7.txt',\n",
       " '72_7.txt',\n",
       " '8976_7.txt',\n",
       " '12105_10.txt',\n",
       " '6447_8.txt',\n",
       " '2819_10.txt',\n",
       " '1011_10.txt',\n",
       " '4731_8.txt',\n",
       " '6890_8.txt',\n",
       " '8933_7.txt',\n",
       " '8170_7.txt',\n",
       " '648_7.txt',\n",
       " '3264_8.txt',\n",
       " '6402_8.txt',\n",
       " '9002_9.txt',\n",
       " '11524_7.txt',\n",
       " '1112_8.txt',\n",
       " '10900_8.txt',\n",
       " '3549_8.txt',\n",
       " '11149_10.txt',\n",
       " '5313_7.txt',\n",
       " '165_7.txt',\n",
       " '926_7.txt',\n",
       " '8694_10.txt',\n",
       " '10143_8.txt',\n",
       " '4059_8.txt',\n",
       " '7065_7.txt',\n",
       " '10592_8.txt',\n",
       " '4261_9.txt',\n",
       " '8372_10.txt',\n",
       " '6490_10.txt',\n",
       " '10106_8.txt',\n",
       " '9114_10.txt',\n",
       " '3734_9.txt',\n",
       " '9552_8.txt',\n",
       " '2297_7.txt',\n",
       " '159_10.txt',\n",
       " '8517_8.txt',\n",
       " '854_9.txt',\n",
       " '2198_8.txt',\n",
       " '2549_8.txt',\n",
       " '11900_8.txt',\n",
       " '10209_7.txt',\n",
       " '11718_10.txt',\n",
       " '7541_10.txt',\n",
       " '3603_7.txt',\n",
       " '7152_9.txt',\n",
       " '2734_9.txt',\n",
       " '6863_7.txt',\n",
       " '9014_10.txt',\n",
       " '10098_10.txt',\n",
       " '6020_7.txt',\n",
       " '5608_9.txt',\n",
       " '7397_8.txt',\n",
       " '2963_8.txt',\n",
       " '4570_10.txt',\n",
       " '11870_10.txt',\n",
       " '2120_8.txt',\n",
       " '8346_9.txt',\n",
       " '11757_9.txt',\n",
       " '4096_10.txt',\n",
       " '2926_8.txt',\n",
       " '5475_8.txt',\n",
       " '10625_7.txt',\n",
       " '7703_8.txt',\n",
       " '1256_8.txt',\n",
       " '4430_8.txt',\n",
       " '11660_7.txt',\n",
       " '11970_10.txt',\n",
       " '6746_8.txt',\n",
       " '3120_8.txt',\n",
       " '8234_7.txt',\n",
       " '6397_8.txt',\n",
       " '4608_9.txt',\n",
       " '3318_9.txt',\n",
       " '4475_8.txt',\n",
       " '11625_7.txt',\n",
       " '6703_8.txt',\n",
       " '9978_8.txt',\n",
       " '1706_9.txt',\n",
       " '4160_9.txt',\n",
       " '10352_10.txt',\n",
       " '9616_8.txt',\n",
       " '10693_8.txt',\n",
       " '7364_7.txt',\n",
       " '10242_8.txt',\n",
       " '264_7.txt',\n",
       " '3648_8.txt',\n",
       " '12134_8.txt',\n",
       " '1743_9.txt',\n",
       " '9282_8.txt',\n",
       " '124_10.txt',\n",
       " '4851_7.txt',\n",
       " '1928_10.txt',\n",
       " '3417_10.txt',\n",
       " '2648_8.txt',\n",
       " '10503_10.txt',\n",
       " '5803_10.txt',\n",
       " '11839_9.txt',\n",
       " '8616_8.txt',\n",
       " '6321_7.txt',\n",
       " '65_10.txt',\n",
       " '782_9.txt',\n",
       " '4057_7.txt',\n",
       " '353_9.txt',\n",
       " '4486_7.txt',\n",
       " '7994_9.txt',\n",
       " '2151_10.txt',\n",
       " '894_9.txt',\n",
       " '2600_10.txt',\n",
       " '3212_7.txt',\n",
       " '10223_10.txt',\n",
       " '11714_10.txt',\n",
       " '1121_7.txt',\n",
       " '704_10.txt',\n",
       " '14_10.txt',\n",
       " '9209_7.txt',\n",
       " '1855_9.txt',\n",
       " '4099_8.txt',\n",
       " '10323_10.txt',\n",
       " '8698_10.txt',\n",
       " '4670_9.txt',\n",
       " '3589_8.txt',\n",
       " '9945_8.txt',\n",
       " '574_7.txt',\n",
       " '3158_8.txt',\n",
       " '2700_10.txt',\n",
       " '9118_10.txt',\n",
       " '6543_9.txt',\n",
       " '10194_10.txt',\n",
       " '3080_10.txt',\n",
       " '155_10.txt',\n",
       " '12088_9.txt',\n",
       " '6192_9.txt',\n",
       " '9592_8.txt',\n",
       " '10517_8.txt',\n",
       " '2257_7.txt',\n",
       " '5136_10.txt',\n",
       " '4320_8.txt',\n",
       " '1546_8.txt',\n",
       " '6815_8.txt',\n",
       " '4350_10.txt',\n",
       " '5667_10.txt',\n",
       " '12109_10.txt',\n",
       " '2815_10.txt',\n",
       " '9413_9.txt',\n",
       " '911_10.txt',\n",
       " '11841_9.txt',\n",
       " '7487_8.txt',\n",
       " '2630_8.txt',\n",
       " '156_8.txt',\n",
       " '7815_8.txt',\n",
       " '12009_10.txt',\n",
       " '811_10.txt',\n",
       " '11047_9.txt',\n",
       " '1298_7.txt',\n",
       " '5281_10.txt',\n",
       " '10135_7.txt',\n",
       " '7850_8.txt',\n",
       " '2688_8.txt',\n",
       " '1924_10.txt',\n",
       " '5398_8.txt',\n",
       " '2061_9.txt',\n",
       " '3950_7.txt',\n",
       " '11038_10.txt',\n",
       " '6756_10.txt',\n",
       " '779_10.txt',\n",
       " '742_9.txt',\n",
       " '69_10.txt',\n",
       " '2024_9.txt',\n",
       " '7642_9.txt',\n",
       " '8242_8.txt',\n",
       " '9065_10.txt',\n",
       " '7787_10.txt',\n",
       " '11616_8.txt',\n",
       " '7430_10.txt',\n",
       " '4749_8.txt',\n",
       " '7361_10.txt',\n",
       " '6656_10.txt',\n",
       " '11138_10.txt',\n",
       " '9483_10.txt',\n",
       " '3688_8.txt',\n",
       " '4571_9.txt',\n",
       " '6607_9.txt',\n",
       " '10282_8.txt',\n",
       " '7730_7.txt',\n",
       " '9693_8.txt',\n",
       " '9165_10.txt',\n",
       " '8303_10.txt',\n",
       " '2915_7.txt',\n",
       " '4534_9.txt',\n",
       " '679_10.txt',\n",
       " '3024_9.txt',\n",
       " '8625_7.txt',\n",
       " '1060_10.txt',\n",
       " '1731_10.txt',\n",
       " '11271_7.txt',\n",
       " '8660_7.txt',\n",
       " '10346_9.txt',\n",
       " '7489_7.txt',\n",
       " '12030_9.txt',\n",
       " '12208_8.txt',\n",
       " '1160_10.txt',\n",
       " '2968_10.txt',\n",
       " '5862_8.txt',\n",
       " '11303_9.txt',\n",
       " '7786_8.txt',\n",
       " '8386_9.txt',\n",
       " '12074_10.txt',\n",
       " '5064_8.txt',\n",
       " '9961_9.txt',\n",
       " '12438_8.txt',\n",
       " '5679_10.txt',\n",
       " '1003_10.txt',\n",
       " '9924_9.txt',\n",
       " '1077_8.txt',\n",
       " '1834_8.txt',\n",
       " '8961_9.txt',\n",
       " '5654_8.txt',\n",
       " '7522_8.txt',\n",
       " '1103_10.txt',\n",
       " '20_9.txt',\n",
       " '1652_10.txt',\n",
       " '5028_10.txt',\n",
       " '2139_9.txt',\n",
       " '5779_10.txt',\n",
       " '2614_9.txt',\n",
       " '7072_9.txt',\n",
       " '7553_10.txt',\n",
       " '11820_8.txt',\n",
       " '4233_7.txt',\n",
       " '1455_7.txt',\n",
       " '9657_10.txt',\n",
       " '6482_10.txt',\n",
       " '4304_9.txt',\n",
       " '6831_9.txt',\n",
       " '9472_8.txt',\n",
       " '7453_10.txt',\n",
       " '2723_7.txt',\n",
       " '6635_10.txt',\n",
       " '7302_10.txt',\n",
       " '7906_7.txt',\n",
       " '1847_10.txt',\n",
       " '11705_7.txt',\n",
       " '9223_9.txt',\n",
       " '469_7.txt',\n",
       " '3806_8.txt',\n",
       " '4184_8.txt',\n",
       " '3000_8.txt',\n",
       " '9266_9.txt',\n",
       " '4333_10.txt',\n",
       " '3238_9.txt',\n",
       " '2876_10.txt',\n",
       " '10705_7.txt',\n",
       " '1079_7.txt',\n",
       " '8223_9.txt',\n",
       " '6569_7.txt',\n",
       " '723_8.txt',\n",
       " '989_9.txt',\n",
       " '2238_9.txt',\n",
       " '11898_8.txt',\n",
       " '2976_10.txt',\n",
       " '11862_10.txt',\n",
       " '12436_7.txt',\n",
       " '3467_7.txt',\n",
       " '8773_8.txt',\n",
       " '273_9.txt',\n",
       " '11291_10.txt',\n",
       " '4132_7.txt',\n",
       " '2768_8.txt',\n",
       " '4971_7.txt',\n",
       " '281_10.txt',\n",
       " '3354_10.txt',\n",
       " '2663_10.txt',\n",
       " '2550_9.txt',\n",
       " '5040_9.txt',\n",
       " '236_9.txt',\n",
       " '7699_10.txt',\n",
       " '1663_9.txt',\n",
       " '2285_10.txt',\n",
       " '136_10.txt',\n",
       " '9736_8.txt',\n",
       " '11126_10.txt',\n",
       " '2032_10.txt',\n",
       " '5971_7.txt',\n",
       " '10362_8.txt',\n",
       " '381_10.txt',\n",
       " '927_10.txt',\n",
       " '7039_7.txt',\n",
       " '4680_10.txt',\n",
       " '12269_8.txt',\n",
       " '8644_7.txt',\n",
       " '5651_10.txt',\n",
       " '4040_8.txt',\n",
       " '11210_7.txt',\n",
       " '5100_10.txt',\n",
       " '8295_7.txt',\n",
       " '4803_8.txt',\n",
       " '5005_8.txt',\n",
       " '7373_8.txt',\n",
       " '7908_9.txt',\n",
       " '808_9.txt',\n",
       " '5751_10.txt',\n",
       " '10210_7.txt',\n",
       " '5000_10.txt',\n",
       " '2923_10.txt',\n",
       " '11362_9.txt',\n",
       " '11595_10.txt',\n",
       " '4895_10.txt',\n",
       " '7623_9.txt',\n",
       " '2045_9.txt',\n",
       " '8519_10.txt',\n",
       " '11677_8.txt',\n",
       " '1241_7.txt',\n",
       " '5184_9.txt',\n",
       " '10215_10.txt',\n",
       " '3301_10.txt',\n",
       " '5982_9.txt',\n",
       " '3931_7.txt',\n",
       " '10578_7.txt',\n",
       " '585_10.txt',\n",
       " '2167_10.txt',\n",
       " '163_10.txt',\n",
       " '7751_7.txt',\n",
       " '10677_8.txt',\n",
       " '3494_9.txt',\n",
       " '632_10.txt',\n",
       " '4555_9.txt',\n",
       " '8348_10.txt',\n",
       " '11495_10.txt',\n",
       " '3201_10.txt',\n",
       " '2172_7.txt',\n",
       " '4728_8.txt',\n",
       " '3550_10.txt',\n",
       " '12195_8.txt',\n",
       " '11622_10.txt',\n",
       " '7338_7.txt',\n",
       " '12350_9.txt',\n",
       " '9917_10.txt',\n",
       " '8500_7.txt',\n",
       " '1707_10.txt',\n",
       " '9437_9.txt',\n",
       " '4341_8.txt',\n",
       " '6874_8.txt',\n",
       " '5848_7.txt',\n",
       " '1056_10.txt',\n",
       " '12315_9.txt',\n",
       " '10063_9.txt',\n",
       " '9500_7.txt',\n",
       " '7831_8.txt',\n",
       " '12042_10.txt',\n",
       " '6875_10.txt',\n",
       " '9545_7.txt',\n",
       " '10111_7.txt',\n",
       " '7874_8.txt',\n",
       " '4848_7.txt',\n",
       " '4726_7.txt',\n",
       " '9702_10.txt',\n",
       " '9053_10.txt',\n",
       " '3236_7.txt',\n",
       " '2344_9.txt',\n",
       " '7257_10.txt',\n",
       " '10539_10.txt',\n",
       " '11533_8.txt',\n",
       " '3344_9.txt',\n",
       " '8268_7.txt',\n",
       " '9602_10.txt',\n",
       " '5726_7.txt',\n",
       " '3795_9.txt',\n",
       " '6386_10.txt',\n",
       " '8464_10.txt',\n",
       " '1812_10.txt',\n",
       " '12294_8.txt',\n",
       " '7887_7.txt',\n",
       " '11679_7.txt',\n",
       " '7357_10.txt',\n",
       " '6131_10.txt',\n",
       " '686_9.txt',\n",
       " '2709_8.txt',\n",
       " '7565_10.txt',\n",
       " '8757_8.txt',\n",
       " '2628_10.txt',\n",
       " '4582_7.txt',\n",
       " '7083_10.txt',\n",
       " '9761_10.txt',\n",
       " '8507_10.txt',\n",
       " '9030_10.txt',\n",
       " '11797_8.txt',\n",
       " '8712_8.txt',\n",
       " '212_9.txt',\n",
       " '6357_9.txt',\n",
       " '9757_8.txt',\n",
       " '2728_10.txt',\n",
       " '1871_10.txt',\n",
       " '2079_10.txt',\n",
       " '11498_7.txt',\n",
       " '6152_10.txt',\n",
       " '7465_10.txt',\n",
       " '9386_8.txt',\n",
       " '10797_8.txt',\n",
       " '4827_9.txt',\n",
       " '1602_9.txt',\n",
       " '8356_10.txt',\n",
       " '11721_7.txt',\n",
       " '4398_9.txt',\n",
       " '1764_10.txt',\n",
       " '1317_8.txt',\n",
       " '939_10.txt',\n",
       " '7508_7.txt',\n",
       " '1783_8.txt',\n",
       " '8330_7.txt',\n",
       " '4278_10.txt',\n",
       " '2259_9.txt',\n",
       " '11829_10.txt',\n",
       " '7607_8.txt',\n",
       " '1664_10.txt',\n",
       " '707_8.txt',\n",
       " '393_8.txt',\n",
       " '8839_8.txt',\n",
       " '6508_7.txt',\n",
       " '11010_10.txt',\n",
       " '5827_10.txt',\n",
       " '10527_10.txt',\n",
       " '10799_7.txt',\n",
       " '4683_7.txt',\n",
       " '10276_10.txt',\n",
       " '11741_10.txt',\n",
       " '11002_8.txt',\n",
       " '8087_8.txt',\n",
       " '6124_7.txt',\n",
       " '2655_10.txt',\n",
       " '2408_8.txt',\n",
       " '6967_7.txt',\n",
       " '7013_9.txt',\n",
       " '8413_8.txt',\n",
       " '2675_9.txt',\n",
       " '950_9.txt',\n",
       " '3707_7.txt',\n",
       " '8881_8.txt',\n",
       " '2755_10.txt',\n",
       " '3533_10.txt',\n",
       " '2004_10.txt',\n",
       " '6056_9.txt',\n",
       " '9456_8.txt',\n",
       " '651_10.txt',\n",
       " '12331_8.txt',\n",
       " '9881_8.txt',\n",
       " '8388_7.txt',\n",
       " '7922_7.txt',\n",
       " '4365_9.txt',\n",
       " '1503_9.txt',\n",
       " '10690_10.txt',\n",
       " '3675_9.txt',\n",
       " '10552_9.txt',\n",
       " '6994_8.txt',\n",
       " '1016_8.txt',\n",
       " '4305_10.txt',\n",
       " '9106_9.txt',\n",
       " '3891_10.txt',\n",
       " '944_10.txt',\n",
       " '1053_8.txt',\n",
       " '10985_9.txt',\n",
       " '5485_10.txt',\n",
       " '11465_7.txt',\n",
       " '406_8.txt',\n",
       " '5670_8.txt',\n",
       " '41_9.txt',\n",
       " '11552_9.txt',\n",
       " '2940_10.txt',\n",
       " '1619_10.txt',\n",
       " '9837_7.txt',\n",
       " '7543_8.txt',\n",
       " '2325_8.txt',\n",
       " '7192_8.txt',\n",
       " '11517_9.txt',\n",
       " '5012_10.txt',\n",
       " '5789_9.txt',\n",
       " '5743_10.txt',\n",
       " '4274_10.txt',\n",
       " '5160_8.txt',\n",
       " '2931_10.txt',\n",
       " '11242_9.txt',\n",
       " '4792_10.txt',\n",
       " '2435_8.txt',\n",
       " '9721_7.txt',\n",
       " '7682_8.txt',\n",
       " '3470_8.txt',\n",
       " '4374_10.txt',\n",
       " '4692_10.txt',\n",
       " '6253_8.txt',\n",
       " '9653_9.txt',\n",
       " '4125_8.txt',\n",
       " '11458_7.txt',\n",
       " '497_10.txt',\n",
       " '3213_10.txt',\n",
       " '7634_7.txt',\n",
       " '10712_8.txt',\n",
       " '2483_7.txt',\n",
       " '7469_10.txt',\n",
       " '10456_10.txt',\n",
       " '5507_7.txt',\n",
       " '620_10.txt',\n",
       " '771_7.txt',\n",
       " '2593_10.txt',\n",
       " '11487_10.txt',\n",
       " '4987_10.txt',\n",
       " '9303_8.txt',\n",
       " '10386_8.txt',\n",
       " '8049_7.txt',\n",
       " '11730_10.txt',\n",
       " '10207_10.txt',\n",
       " '7569_10.txt',\n",
       " '10089_7.txt',\n",
       " '10556_10.txt',\n",
       " '646_9.txt',\n",
       " '1324_7.txt',\n",
       " '3442_10.txt',\n",
       " '3313_10.txt',\n",
       " '39_9.txt',\n",
       " '8797_8.txt',\n",
       " '11587_10.txt',\n",
       " '4887_10.txt',\n",
       " '11386_8.txt',\n",
       " '4507_7.txt',\n",
       " '2493_10.txt',\n",
       " '3017_7.txt',\n",
       " '9465_7.txt',\n",
       " '8985_9.txt',\n",
       " '12050_10.txt',\n",
       " '10872_7.txt',\n",
       " '9805_10.txt',\n",
       " '483_8.txt',\n",
       " '11945_9.txt',\n",
       " '10889_10.txt',\n",
       " '12302_7.txt',\n",
       " '6218_7.txt',\n",
       " '8552_9.txt',\n",
       " '6954_8.txt',\n",
       " '9985_9.txt',\n",
       " '11837_7.txt',\n",
       " '948_10.txt',\n",
       " '11074_7.txt',\n",
       " '1093_8.txt',\n",
       " '10945_9.txt',\n",
       " '5489_10.txt',\n",
       " '9905_10.txt',\n",
       " '5292_7.txt',\n",
       " '2759_10.txt',\n",
       " '3019_8.txt',\n",
       " '6672_10.txt',\n",
       " '5643_7.txt',\n",
       " '8190_10.txt',\n",
       " '3788_10.txt',\n",
       " '1951_9.txt',\n",
       " '6394_10.txt',\n",
       " '8476_10.txt',\n",
       " '9610_10.txt',\n",
       " '196_9.txt',\n",
       " '6772_10.txt',\n",
       " '7245_10.txt',\n",
       " '10388_7.txt',\n",
       " '4292_7.txt',\n",
       " '2108_10.txt',\n",
       " '990_9.txt',\n",
       " '1823_7.txt',\n",
       " '6294_10.txt',\n",
       " '4606_7.txt',\n",
       " '9799_7.txt',\n",
       " '9710_10.txt',\n",
       " '8841_8.txt',\n",
       " '9348_7.txt',\n",
       " '2264_9.txt',\n",
       " '11087_8.txt',\n",
       " '9632_8.txt',\n",
       " '11499_10.txt',\n",
       " '8344_10.txt',\n",
       " '12110_8.txt',\n",
       " '3411_9.txt',\n",
       " '1863_10.txt',\n",
       " '7326_10.txt',\n",
       " '11169_7.txt',\n",
       " '5073_7.txt',\n",
       " '10448_10.txt',\n",
       " '3629_8.txt',\n",
       " '9022_10.txt',\n",
       " '4875_7.txt',\n",
       " '4899_10.txt',\n",
       " '4036_7.txt',\n",
       " '949_8.txt',\n",
       " '6040_10.txt',\n",
       " '10219_10.txt',\n",
       " '5339_8.txt',\n",
       " '9295_10.txt',\n",
       " '10169_7.txt',\n",
       " '6711_10.txt',\n",
       " '2883_9.txt',\n",
       " '589_10.txt',\n",
       " '11858_9.txt',\n",
       " '2411_9.txt',\n",
       " '10295_7.txt',\n",
       " '11736_9.txt',\n",
       " '662_8.txt',\n",
       " '7762_8.txt',\n",
       " '1127_10.txt',\n",
       " '5451_8.txt',\n",
       " '627_8.txt',\n",
       " '8327_9.txt',\n",
       " '8210_7.txt',\n",
       " '4886_8.txt',\n",
       " '9966_10.txt',\n",
       " '1237_8.txt',\n",
       " '1776_10.txt',\n",
       " '3590_8.txt',\n",
       " '10773_9.txt',\n",
       " '643_10.txt',\n",
       " '7041_7.txt',\n",
       " '112_10.txt',\n",
       " '10682_10.txt',\n",
       " '8339_10.txt',\n",
       " '5372_7.txt',\n",
       " '3710_9.txt',\n",
       " '6176_9.txt',\n",
       " '10364_10.txt',\n",
       " '11653_10.txt',\n",
       " '4200_9.txt',\n",
       " '11102_10.txt',\n",
       " '8533_8.txt',\n",
       " '8239_10.txt',\n",
       " '5694_9.txt',\n",
       " '4337_7.txt',\n",
       " '53_10.txt',\n",
       " '8576_8.txt',\n",
       " '10264_10.txt',\n",
       " '11753_10.txt',\n",
       " '5835_10.txt',\n",
       " '3421_10.txt',\n",
       " '2116_10.txt',\n",
       " '11961_8.txt',\n",
       " '1514_7.txt',\n",
       " '2647_10.txt',\n",
       " '2205_8.txt',\n",
       " '10545_7.txt',\n",
       " '5597_10.txt',\n",
       " '3983_10.txt',\n",
       " '11437_9.txt',\n",
       " '8026_9.txt',\n",
       " '11846_10.txt',\n",
       " '5071_10.txt',\n",
       " '4546_10.txt',\n",
       " '5750_8.txt',\n",
       " '6879_10.txt',\n",
       " '5720_10.txt',\n",
       " '2078_9.txt',\n",
       " '2691_8.txt',\n",
       " '9154_7.txt',\n",
       " '3883_10.txt',\n",
       " '8952_7.txt',\n",
       " '6463_8.txt',\n",
       " '3205_8.txt',\n",
       " '1173_8.txt',\n",
       " '10997_10.txt',\n",
       " '7729_7.txt',\n",
       " '8917_7.txt',\n",
       " '13_7.txt',\n",
       " '3691_8.txt',\n",
       " '6426_8.txt',\n",
       " '9026_9.txt',\n",
       " '5171_10.txt',\n",
       " '4446_10.txt',\n",
       " '1136_8.txt',\n",
       " '4317_10.txt',\n",
       " '5620_10.txt',\n",
       " '6979_10.txt',\n",
       " '9806_8.txt',\n",
       " '4733_9.txt',\n",
       " '8159_10.txt',\n",
       " '423_10.txt',\n",
       " '1584_9.txt',\n",
       " '5641_7.txt',\n",
       " '372_10.txt',\n",
       " '2790_10.txt',\n",
       " '9188_10.txt',\n",
       " '1110_9.txt',\n",
       " '10104_10.txt',\n",
       " '6400_9.txt',\n",
       " '2314_7.txt',\n",
       " '523_10.txt',\n",
       " '11411_8.txt',\n",
       " '8494_8.txt',\n",
       " '6537_7.txt',\n",
       " '272_10.txt',\n",
       " '11883_8.txt',\n",
       " '11784_10.txt',\n",
       " '8045_8.txt',\n",
       " '8059_10.txt',\n",
       " '738_8.txt',\n",
       " '12468_7.txt',\n",
       " '2427_10.txt',\n",
       " '84_10.txt',\n",
       " '4833_10.txt',\n",
       " '500_9.txt',\n",
       " '7400_9.txt',\n",
       " '10755_10.txt',\n",
       " '11902_9.txt',\n",
       " '8987_9.txt',\n",
       " '5263_8.txt',\n",
       " '7581_8.txt',\n",
       " '8768_8.txt',\n",
       " '11947_9.txt',\n",
       " '4077_10.txt',\n",
       " '5540_10.txt',\n",
       " '6956_8.txt',\n",
       " '4263_8.txt',\n",
       " '11991_10.txt',\n",
       " '4491_10.txt',\n",
       " '10141_9.txt',\n",
       " '8467_7.txt',\n",
       " '9987_9.txt',\n",
       " '4818_9.txt',\n",
       " '3736_8.txt',\n",
       " '10940_10.txt',\n",
       " '5440_10.txt',\n",
       " '3854_10.txt',\n",
       " '1678_9.txt',\n",
       " '6197_10.txt',\n",
       " '2050_7.txt',\n",
       " '6395_9.txt',\n",
       " '9344_8.txt',\n",
       " '6320_10.txt',\n",
       " '773_7.txt',\n",
       " '7673_7.txt',\n",
       " '7146_10.txt',\n",
       " '6471_10.txt',\n",
       " '6701_9.txt',\n",
       " '644_9.txt',\n",
       " '4540_7.txt',\n",
       " '6097_10.txt',\n",
       " '3813_7.txt',\n",
       " '9242_10.txt',\n",
       " '9513_10.txt',\n",
       " '8024_10.txt',\n",
       " '7717_10.txt',\n",
       " '6673_7.txt',\n",
       " '6571_10.txt',\n",
       " '10728_10.txt',\n",
       " '8614_9.txt',\n",
       " '12044_7.txt',\n",
       " '12495_7.txt',\n",
       " '1347_10.txt',\n",
       " '6958_7.txt',\n",
       " '5964_8.txt',\n",
       " '8651_9.txt',\n",
       " '1416_10.txt',\n",
       " '7802_10.txt',\n",
       " '1899_7.txt',\n",
       " '7680_8.txt',\n",
       " '12136_9.txt',\n",
       " '8960_10.txt',\n",
       " '4921_8.txt',\n",
       " '1704_8.txt',\n",
       " '10078_8.txt',\n",
       " '7902_10.txt',\n",
       " '9280_9.txt',\n",
       " '3829_10.txt',\n",
       " '3437_8.txt',\n",
       " '9651_9.txt',\n",
       " '4127_8.txt',\n",
       " '8688_8.txt',\n",
       " '7861_10.txt',\n",
       " '4069_10.txt',\n",
       " '5752_8.txt',\n",
       " '8803_10.txt',\n",
       " '11299_8.txt',\n",
       " '10990_7.txt',\n",
       " '12087_10.txt',\n",
       " '7961_10.txt',\n",
       " '8950_7.txt',\n",
       " '4638_10.txt',\n",
       " '11196_7.txt',\n",
       " '1798_9.txt',\n",
       " '12392_9.txt',\n",
       " '4717_8.txt',\n",
       " '10470_9.txt',\n",
       " '3693_8.txt',\n",
       " '3242_8.txt',\n",
       " '8587_7.txt',\n",
       " '10648_8.txt',\n",
       " '6309_8.txt',\n",
       " '7674_10.txt',\n",
       " '6343_10.txt',\n",
       " '143_7.txt',\n",
       " '9196_10.txt',\n",
       " '3386_9.txt',\n",
       " '7043_7.txt',\n",
       " '7125_10.txt',\n",
       " '7492_7.txt',\n",
       " '2268_10.txt',\n",
       " '2539_10.txt',\n",
       " '6972_9.txt',\n",
       " '1421_9.txt',\n",
       " '5370_7.txt',\n",
       " '8147_10.txt',\n",
       " '7131_9.txt',\n",
       " '872_9.txt',\n",
       " '9096_10.txt',\n",
       " '11165_8.txt',\n",
       " '12159_7.txt',\n",
       " '209_8.txt',\n",
       " '7174_9.txt',\n",
       " '11318_9.txt',\n",
       " '1516_7.txt',\n",
       " '8716_10.txt',\n",
       " '7292_10.txt',\n",
       " '4014_10.txt',\n",
       " '8360_9.txt',\n",
       " '9257_7.txt',\n",
       " '11198_8.txt',\n",
       " '5453_8.txt',\n",
       " '5794_10.txt',\n",
       " '9686_7.txt',\n",
       " '8325_9.txt',\n",
       " '3106_8.txt',\n",
       " '8212_7.txt',\n",
       " '11297_7.txt',\n",
       " '10198_8.txt',\n",
       " '11450_10.txt',\n",
       " '7609_10.txt',\n",
       " '3456_9.txt',\n",
       " '7342_7.txt',\n",
       " '1518_8.txt',\n",
       " '3722_10.txt',\n",
       " '5034_7.txt',\n",
       " '6189_10.txt',\n",
       " '9675_8.txt',\n",
       " '2561_7.txt',\n",
       " '10221_8.txt',\n",
       " '3595_10.txt',\n",
       " '440_10.txt',\n",
       " '3881_9.txt',\n",
       " '2315_10.txt',\n",
       " '7008_8.txt',\n",
       " '4850_10.txt',\n",
       " '5146_9.txt',\n",
       " '10736_10.txt',\n",
       " '5905_9.txt',\n",
       " '10928_7.txt',\n",
       " '375_9.txt',\n",
       " '6089_10.txt',\n",
       " '10581_10.txt',\n",
       " '8675_8.txt',\n",
       " '4724_7.txt',\n",
       " '96_10.txt',\n",
       " '3653_10.txt',\n",
       " '2364_10.txt',\n",
       " '1901_7.txt',\n",
       " '2346_9.txt',\n",
       " '11270_10.txt',\n",
       " '10747_10.txt',\n",
       " '8963_8.txt',\n",
       " '4821_10.txt',\n",
       " '11521_10.txt',\n",
       " '7778_10.txt',\n",
       " '531_10.txt',\n",
       " '6417_7.txt',\n",
       " '8165_8.txt',\n",
       " '22_8.txt',\n",
       " '5613_9.txt',\n",
       " '7129_10.txt',\n",
       " '7678_10.txt',\n",
       " '10116_10.txt',\n",
       " '11421_10.txt',\n",
       " '9318_9.txt',\n",
       " '3797_9.txt',\n",
       " '985_7.txt',\n",
       " '12296_8.txt',\n",
       " '2782_10.txt',\n",
       " '360_10.txt',\n",
       " '6070_8.txt',\n",
       " '10952_10.txt',\n",
       " '9299_8.txt',\n",
       " '5303_10.txt',\n",
       " '4634_10.txt',\n",
       " '9648_8.txt',\n",
       " '1579_10.txt',\n",
       " '3846_10.txt',\n",
       " '10024_9.txt',\n",
       " '10259_8.txt',\n",
       " '6876_8.txt',\n",
       " '11983_10.txt',\n",
       " '2897_10.txt',\n",
       " '3282_8.txt',\n",
       " '7248_9.txt',\n",
       " '1328_10.txt',\n",
       " '11024_9.txt',\n",
       " '4065_10.txt',\n",
       " '10852_10.txt',\n",
       " '7833_8.txt',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_files = os.listdir('/Users/zouhao/Desktop/LSTM情感分析Test4/pos') #得到文件夹下的所有文件名称\n",
    "neg_files = os.listdir('/Users/zouhao/Desktop/LSTM情感分析Test4/neg') #得到文件夹下的所有文件名称\n",
    "pos_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正面评价完结\n"
     ]
    }
   ],
   "source": [
    "num_words = []\n",
    "for pf in pos_files:\n",
    "    with open('/Users/zouhao/Desktop/LSTM情感分析Test4/pos/'+pf, \"r\", encoding = 'utf-8') as f:\n",
    "        line = f.readline()\n",
    "        counter = len(line.split())\n",
    "        num_words.append(counter)\n",
    "print('正面评价完结')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "负面评价完结\n"
     ]
    }
   ],
   "source": [
    "for nf in neg_files:\n",
    "    with open('/Users/zouhao/Desktop/LSTM情感分析Test4/neg/'+nf, \"r\", encoding='utf-8') as f:\n",
    "        line = f.readline()\n",
    "        counter = len(line.split())#也可用collections.Counter()来统计词频 \n",
    "        num_words.append(counter)\n",
    "print('负面评价完结')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件总数 25000\n",
      "所有的词的数量 5844680\n",
      "平均文件词的长度 233.7872\n"
     ]
    }
   ],
   "source": [
    "num_files = len(num_words)\n",
    "print('文件总数', num_files)\n",
    "print('所有的词的数量', sum(num_words))\n",
    "print('平均文件词的长度', sum(num_words) / len(num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_num = 250\n",
    "num_dimensions = 50  # 每个单词向量的维度，这里和嵌入矩阵的每个词的维度相同\n",
    "\n",
    "# arr:24 x 250的矩阵\n",
    "def get_train_batch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batch_size, max_seq_num])\n",
    "    for i in range(batch_size):\n",
    "        if (i % 2 == 0):\n",
    "            num = randint(1, 11499)\n",
    "            labels.append([1, 0])\n",
    "        else:\n",
    "            num = randint(13499, 24999)\n",
    "            labels.append([0, 1])\n",
    "        arr[i] = ids[num - 1:num]\n",
    "    return arr, labels\n",
    "\n",
    "# 同上\n",
    "def get_test_batch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batch_size, max_seq_num])\n",
    "    for i in range(batch_size):\n",
    "        num = randint(11499, 13499)\n",
    "        if (num <= 12499):\n",
    "            labels.append([1, 0])\n",
    "        else:\n",
    "            labels.append([0, 1])\n",
    "        arr[i] = ids[num - 1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "载入IDS: (25000, 250)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 24 # batch的尺寸\n",
    "lstm_units = 64 # lstm的单元数量\n",
    "num_labels = 2  # 输出的类别数\n",
    "iterations = 200000 # 迭代的次数 \n",
    "# 载入正负样本的词典映射\n",
    "ids = np.load('/Users/zouhao/Desktop/LSTM情感分析Test4/idsMatrix.npy')\n",
    "print('载入IDS:',ids.shape)\n",
    "tf.reset_default_graph()\n",
    "# 确定好单元的占位符：输入是24x300，输出是24x2\n",
    "labels = tf.placeholder(tf.float32, [batch_size, num_labels])\n",
    "input_data = tf.placeholder(tf.int32, [batch_size, max_seq_num])\n",
    "\n",
    "# 必须先定义该变量\n",
    "data = tf.Variable(\n",
    "    tf.zeros([batch_size, max_seq_num, num_dimensions]), dtype=tf.float32)\n",
    "# 调用tf.nn.lookup()接口获得文本向量，该函数返回batch_size个文本的3D张量，用于后续的训练\n",
    "data = tf.nn.embedding_lookup(wordVectors, input_data)\n",
    "\n",
    "# 使用tf.contrib.rnn.BasicLSTMCell细胞单元配置lstm的数量\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstm_units)\n",
    "# 配置dropout参数，以此避免过拟合\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "# 最后将LSTM cell和数据输入到tf.nn.dynamic_rnn函数，功能是展开整个网络，并且构建一整个RNN模型\n",
    "# 这里的value认为是最后的隐藏状态，该向量将重新确定维度，然后乘以一个权重加上偏置，最终获得lable\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([lstm_units, num_labels]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[num_labels]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1、使用tf.train.Saver.save()方法保存模型\n",
    "\n",
    "    sess: 用于保存变量操作的会话。\n",
    "\n",
    "    save_path: String类型，用于指定训练结果的保存路径。\n",
    "\n",
    "    global_step: 如果提供的话，这个数字会添加到save_path后面，用于构建checkpoint文件。这个参数有助于我们区分不同训练阶段的结果。\n",
    "\n",
    "    2、使用tf.train.Saver.restore方法价值模型\n",
    "\n",
    "        sess: 用于加载变量操作的会话。\n",
    "\n",
    "        save_path: 同保存模型是用到的的save_path参数。\n",
    "\n",
    "        下面通过一个代码演示这两个函数的使用方法\n",
    "\n",
    "    假设保存变量的时候是\n",
    "\n",
    "checkpoint_filepath='models/train.ckpt'\n",
    "saver.save(session,checkpoint_filepath)\n",
    "则从文件读变量取值继续训练是\n",
    "saver.restore(session,checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3、saver=tf.train.Saver()\n",
    "\n",
    "假设保存变量的时候是\n",
    "checkpoint_filepath='models/train.ckpt'\n",
    "saver.save(session,checkpoint_filepath)\n",
    "则从文件读变量取值继续训练是\n",
    "saver.restore(session,checkpoint_filepath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:1/10000 \n",
      "loss:0.7354393005371094 \n",
      "accuracy:62.5\n",
      "..........\n",
      "iteration:1001/10000 \n",
      "loss:0.5787484645843506 \n",
      "accuracy:79.16666865348816\n",
      "..........\n",
      "iteration:2001/10000 \n",
      "loss:0.6139519810676575 \n",
      "accuracy:62.5\n",
      "..........\n",
      "iteration:3001/10000 \n",
      "loss:0.7065317630767822 \n",
      "accuracy:50.0\n",
      "..........\n",
      "iteration:4001/10000 \n",
      "loss:0.6635363698005676 \n",
      "accuracy:41.66666567325592\n",
      "..........\n",
      "iteration:5001/10000 \n",
      "loss:0.7302400469779968 \n",
      "accuracy:50.0\n",
      "..........\n",
      "iteration:6001/10000 \n",
      "loss:0.6600983738899231 \n",
      "accuracy:50.0\n",
      "..........\n",
      "iteration:7001/10000 \n",
      "loss:0.6846897006034851 \n",
      "accuracy:66.66666865348816\n",
      "..........\n",
      "iteration:8001/10000 \n",
      "loss:0.643425464630127 \n",
      "accuracy:62.5\n",
      "..........\n",
      "iteration:9001/10000 \n",
      "loss:0.6095423698425293 \n",
      "accuracy:75.0\n",
      "..........\n"
     ]
    }
   ],
   "source": [
    "# 定义正确的预测函数和正确率评估参数\n",
    "correctPred = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "# 最后将标准的交叉熵损失函数定义为损失值，这里是以adam为优化函数\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "tf.summary.scalar('loss',loss)\n",
    "tf.summary.scalar('Accrar',accuracy)\n",
    "merged=tf.summary.merge_all()\n",
    "logdir='tensorboard/'+ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"/\"\n",
    "writer=tf.summary.FileWriter(logdir,sess.graph)\n",
    "\n",
    "\n",
    "iterations = 10000\n",
    "for i in range(iterations):\n",
    "    # 下个批次的数据\n",
    "    next_batch, next_batch_labels = get_train_batch()\n",
    "    sess.run(optimizer,{input_data: next_batch, labels: next_batch_labels}) \n",
    "\n",
    "    # 每50次写入一次leadboard\n",
    "    if(i%50==0):\n",
    "        summary=sess.run(merged,{input_data: next_batch, labels: next_batch_labels})\n",
    "        writer.add_summary(summary,i)\n",
    "\n",
    "    if (i%1000==0):\n",
    "        loss_ = sess.run(loss, {input_data: next_batch, labels: next_batch_labels})\n",
    "\n",
    "        accuracy_=(sess.run(accuracy, {input_data: next_batch, labels: next_batch_labels})) * 100\n",
    "        print(\"iteration:{}/{}\".format(i+1, iterations),\n",
    "                  \"\\nloss:{}\".format(loss_),\n",
    "                  \"\\naccuracy:{}\".format(accuracy_))    \n",
    "        print('..........')  \n",
    "    # 每10000次保存一下模型\n",
    "    if(i%10000==0 and i!=0):\n",
    "        save_path=saver.save(sess,\"/Users/zouhao/Desktop/LSTM情感分析Test4/models/pretrained_lstm.ckpt\",global_step=i)\n",
    "        print(\"saved to %s\"% save_path)\n",
    "\n",
    "writer.close()#4:55-5:40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:1/1000 \n",
      "loss:0.835259735584259 \n",
      "accuracy:54.16666865348816\n",
      "..........\n",
      "iteration:101/1000 \n",
      "loss:0.69843989610672 \n",
      "accuracy:50.0\n",
      "..........\n",
      "iteration:201/1000 \n",
      "loss:0.6276010870933533 \n",
      "accuracy:62.5\n",
      "..........\n",
      "iteration:301/1000 \n",
      "loss:0.7385787963867188 \n",
      "accuracy:66.66666865348816\n",
      "..........\n",
      "iteration:401/1000 \n",
      "loss:0.623770534992218 \n",
      "accuracy:54.16666865348816\n",
      "..........\n",
      "iteration:501/1000 \n",
      "loss:0.7822547554969788 \n",
      "accuracy:54.16666865348816\n",
      "..........\n",
      "iteration:601/1000 \n",
      "loss:0.7419171333312988 \n",
      "accuracy:54.16666865348816\n",
      "..........\n",
      "iteration:701/1000 \n",
      "loss:0.6957378387451172 \n",
      "accuracy:66.66666865348816\n",
      "..........\n",
      "iteration:801/1000 \n",
      "loss:0.6850967407226562 \n",
      "accuracy:33.33333432674408\n",
      "..........\n",
      "iteration:901/1000 \n",
      "loss:0.632957398891449 \n",
      "accuracy:62.5\n",
      "..........\n"
     ]
    }
   ],
   "source": [
    "# 定义正确的预测函数和正确率评估参数\n",
    "correctPred = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "# 最后将标准的交叉熵损失函数定义为损失值，这里是以adam为优化函数\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "tf.summary.scalar('loss',loss)\n",
    "tf.summary.scalar('Accrar',accuracy)\n",
    "merged=tf.summary.merge_all()\n",
    "logdir='tensorboard/'+ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"/\"\n",
    "writer=tf.summary.FileWriter(logdir,sess.graph)\n",
    "\n",
    "\n",
    "iterations = 1000\n",
    "for i in range(iterations):\n",
    "    # 下个批次的数据\n",
    "    next_batch, next_batch_labels = get_train_batch()\n",
    "    sess.run(optimizer,{input_data: next_batch, labels: next_batch_labels}) \n",
    "\n",
    "    # 每50次写入一次leadboard\n",
    "    if(i%50==0):\n",
    "        summary=sess.run(merged,{input_data: next_batch, labels: next_batch_labels})\n",
    "        writer.add_summary(summary,i)\n",
    "\n",
    "    if (i%100==0):\n",
    "        loss_ = sess.run(loss, {input_data: next_batch, labels: next_batch_labels})\n",
    "\n",
    "        accuracy_=(sess.run(accuracy, {input_data: next_batch, labels: next_batch_labels})) * 100\n",
    "        print(\"iteration:{}/{}\".format(i+1, iterations),\n",
    "                  \"\\nloss:{}\".format(loss_),\n",
    "                  \"\\naccuracy:{}\".format(accuracy_))    \n",
    "        print('..........')  \n",
    "    # 每1000次保存一下模型\n",
    "    if(i%1000==0 and i!=0):\n",
    "        save_path=saver.save(sess,\"/Users/zouhao/Desktop/LSTM情感分析Test4/models/pretrained_lstm.ckpt\",global_step=i)\n",
    "        print(\"saved to %s\"% save_path)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:1/100000 \n",
      "loss:0.8111233711242676 \n",
      "accuracy:54.16666865348816\n",
      "..........\n",
      "iteration:1001/100000 \n",
      "loss:0.6626638770103455 \n",
      "accuracy:58.33333134651184\n",
      "..........\n",
      "iteration:2001/100000 \n",
      "loss:0.7344972491264343 \n",
      "accuracy:45.83333432674408\n",
      "..........\n",
      "iteration:3001/100000 \n",
      "loss:0.7305406928062439 \n",
      "accuracy:54.16666865348816\n",
      "..........\n",
      "iteration:4001/100000 \n",
      "loss:0.6955063939094543 \n",
      "accuracy:50.0\n",
      "..........\n",
      "iteration:5001/100000 \n",
      "loss:0.5072590708732605 \n",
      "accuracy:83.33333134651184\n",
      "..........\n",
      "iteration:6001/100000 \n",
      "loss:0.5338289141654968 \n",
      "accuracy:79.16666865348816\n",
      "..........\n",
      "iteration:7001/100000 \n",
      "loss:0.3405703604221344 \n",
      "accuracy:79.16666865348816\n",
      "..........\n",
      "iteration:8001/100000 \n",
      "loss:0.48696961998939514 \n",
      "accuracy:75.0\n",
      "..........\n",
      "iteration:9001/100000 \n",
      "loss:0.3967016041278839 \n",
      "accuracy:83.33333134651184\n",
      "..........\n",
      "iteration:10001/100000 \n",
      "loss:0.4368619918823242 \n",
      "accuracy:83.33333134651184\n",
      "..........\n",
      "saved to /Users/zouhao/Desktop/LSTM情感分析Test4/models/pretrained_lstm.ckpt-10000\n",
      "iteration:11001/100000 \n",
      "loss:0.46186354756355286 \n",
      "accuracy:79.16666865348816\n",
      "..........\n",
      "iteration:12001/100000 \n",
      "loss:0.44259822368621826 \n",
      "accuracy:87.5\n",
      "..........\n",
      "iteration:13001/100000 \n",
      "loss:0.2822093665599823 \n",
      "accuracy:91.66666865348816\n",
      "..........\n",
      "iteration:14001/100000 \n",
      "loss:0.13795879483222961 \n",
      "accuracy:95.83333134651184\n",
      "..........\n",
      "iteration:15001/100000 \n",
      "loss:0.3096757233142853 \n",
      "accuracy:91.66666865348816\n",
      "..........\n",
      "iteration:16001/100000 \n",
      "loss:0.31238681077957153 \n",
      "accuracy:87.5\n",
      "..........\n",
      "iteration:17001/100000 \n",
      "loss:0.29848888516426086 \n",
      "accuracy:83.33333134651184\n",
      "..........\n",
      "iteration:18001/100000 \n",
      "loss:0.21897752583026886 \n",
      "accuracy:87.5\n",
      "..........\n",
      "iteration:19001/100000 \n",
      "loss:0.22556328773498535 \n",
      "accuracy:87.5\n",
      "..........\n",
      "iteration:20001/100000 \n",
      "loss:0.11523586511611938 \n",
      "accuracy:95.83333134651184\n",
      "..........\n",
      "saved to /Users/zouhao/Desktop/LSTM情感分析Test4/models/pretrained_lstm.ckpt-20000\n",
      "iteration:21001/100000 \n",
      "loss:0.18387837707996368 \n",
      "accuracy:95.83333134651184\n",
      "..........\n",
      "iteration:22001/100000 \n",
      "loss:0.09636878967285156 \n",
      "accuracy:95.83333134651184\n",
      "..........\n",
      "iteration:23001/100000 \n",
      "loss:0.05092369392514229 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:24001/100000 \n",
      "loss:0.2128036469221115 \n",
      "accuracy:83.33333134651184\n",
      "..........\n",
      "iteration:25001/100000 \n",
      "loss:0.1961967498064041 \n",
      "accuracy:91.66666865348816\n",
      "..........\n",
      "iteration:26001/100000 \n",
      "loss:0.05356419086456299 \n",
      "accuracy:95.83333134651184\n",
      "..........\n",
      "iteration:27001/100000 \n",
      "loss:0.1696070283651352 \n",
      "accuracy:95.83333134651184\n",
      "..........\n",
      "iteration:28001/100000 \n",
      "loss:0.013163115829229355 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:29001/100000 \n",
      "loss:0.11017260700464249 \n",
      "accuracy:95.83333134651184\n",
      "..........\n",
      "iteration:30001/100000 \n",
      "loss:0.04657784476876259 \n",
      "accuracy:100.0\n",
      "..........\n",
      "saved to /Users/zouhao/Desktop/LSTM情感分析Test4/models/pretrained_lstm.ckpt-30000\n",
      "iteration:31001/100000 \n",
      "loss:0.014483276754617691 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:32001/100000 \n",
      "loss:0.08515483886003494 \n",
      "accuracy:95.83333134651184\n",
      "..........\n",
      "iteration:33001/100000 \n",
      "loss:0.09580785781145096 \n",
      "accuracy:95.83333134651184\n",
      "..........\n",
      "iteration:34001/100000 \n",
      "loss:0.015704715624451637 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:35001/100000 \n",
      "loss:0.013267911970615387 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:36001/100000 \n",
      "loss:0.035587605088949203 \n",
      "accuracy:95.83333134651184\n",
      "..........\n",
      "iteration:37001/100000 \n",
      "loss:0.027825841680169106 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:38001/100000 \n",
      "loss:0.02190331555902958 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:39001/100000 \n",
      "loss:0.013422980904579163 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:40001/100000 \n",
      "loss:0.040584053844213486 \n",
      "accuracy:100.0\n",
      "..........\n",
      "saved to /Users/zouhao/Desktop/LSTM情感分析Test4/models/pretrained_lstm.ckpt-40000\n",
      "iteration:41001/100000 \n",
      "loss:0.01955067180097103 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:42001/100000 \n",
      "loss:0.021578408777713776 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:43001/100000 \n",
      "loss:0.003843021346256137 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:44001/100000 \n",
      "loss:0.025291532278060913 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:45001/100000 \n",
      "loss:0.007328371983021498 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:46001/100000 \n",
      "loss:0.0034099689219146967 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:47001/100000 \n",
      "loss:0.017986582592129707 \n",
      "accuracy:95.83333134651184\n",
      "..........\n",
      "iteration:48001/100000 \n",
      "loss:0.006877038162201643 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:49001/100000 \n",
      "loss:0.00889150332659483 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:50001/100000 \n",
      "loss:0.041235629469156265 \n",
      "accuracy:100.0\n",
      "..........\n",
      "saved to /Users/zouhao/Desktop/LSTM情感分析Test4/models/pretrained_lstm.ckpt-50000\n",
      "iteration:51001/100000 \n",
      "loss:0.007235642522573471 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:52001/100000 \n",
      "loss:0.006288837641477585 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:53001/100000 \n",
      "loss:0.03305220231413841 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:54001/100000 \n",
      "loss:0.010270665399730206 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:55001/100000 \n",
      "loss:0.0300883948802948 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:56001/100000 \n",
      "loss:0.007796993479132652 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:57001/100000 \n",
      "loss:0.002366002881899476 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:58001/100000 \n",
      "loss:0.002652158960700035 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:59001/100000 \n",
      "loss:0.02065763995051384 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:60001/100000 \n",
      "loss:0.005752153694629669 \n",
      "accuracy:100.0\n",
      "..........\n",
      "saved to /Users/zouhao/Desktop/LSTM情感分析Test4/models/pretrained_lstm.ckpt-60000\n",
      "iteration:61001/100000 \n",
      "loss:0.003834324888885021 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:62001/100000 \n",
      "loss:0.004275956191122532 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:63001/100000 \n",
      "loss:0.003568194108083844 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:64001/100000 \n",
      "loss:0.10407143086194992 \n",
      "accuracy:95.83333134651184\n",
      "..........\n",
      "iteration:65001/100000 \n",
      "loss:0.004372339230030775 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:66001/100000 \n",
      "loss:0.014287080615758896 \n",
      "accuracy:95.83333134651184\n",
      "..........\n",
      "iteration:67001/100000 \n",
      "loss:0.051275596022605896 \n",
      "accuracy:95.83333134651184\n",
      "..........\n",
      "iteration:68001/100000 \n",
      "loss:0.003276551840826869 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:69001/100000 \n",
      "loss:0.005698196589946747 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:70001/100000 \n",
      "loss:0.01435967069119215 \n",
      "accuracy:100.0\n",
      "..........\n",
      "saved to /Users/zouhao/Desktop/LSTM情感分析Test4/models/pretrained_lstm.ckpt-70000\n",
      "iteration:71001/100000 \n",
      "loss:0.09732717275619507 \n",
      "accuracy:95.83333134651184\n",
      "..........\n",
      "iteration:72001/100000 \n",
      "loss:0.007999824360013008 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:73001/100000 \n",
      "loss:0.02657393552362919 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:74001/100000 \n",
      "loss:0.004995220806449652 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:75001/100000 \n",
      "loss:0.01348329707980156 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:76001/100000 \n",
      "loss:0.01850377954542637 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:77001/100000 \n",
      "loss:0.011865980923175812 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:78001/100000 \n",
      "loss:0.011012382805347443 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:79001/100000 \n",
      "loss:0.0015686951810494065 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:80001/100000 \n",
      "loss:0.0520096980035305 \n",
      "accuracy:100.0\n",
      "..........\n",
      "saved to /Users/zouhao/Desktop/LSTM情感分析Test4/models/pretrained_lstm.ckpt-80000\n",
      "iteration:81001/100000 \n",
      "loss:0.004720732569694519 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:82001/100000 \n",
      "loss:0.0021851013880223036 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:83001/100000 \n",
      "loss:0.005278558935970068 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:84001/100000 \n",
      "loss:0.0018680752255022526 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:85001/100000 \n",
      "loss:0.003953825682401657 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:86001/100000 \n",
      "loss:0.03838392347097397 \n",
      "accuracy:95.83333134651184\n",
      "..........\n",
      "iteration:87001/100000 \n",
      "loss:0.006312536541372538 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:88001/100000 \n",
      "loss:0.007939697243273258 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:89001/100000 \n",
      "loss:0.14382599294185638 \n",
      "accuracy:95.83333134651184\n",
      "..........\n",
      "iteration:90001/100000 \n",
      "loss:0.0038257900159806013 \n",
      "accuracy:100.0\n",
      "..........\n",
      "saved to /Users/zouhao/Desktop/LSTM情感分析Test4/models/pretrained_lstm.ckpt-90000\n",
      "iteration:91001/100000 \n",
      "loss:0.013442509807646275 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:92001/100000 \n",
      "loss:0.0017965348670259118 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:93001/100000 \n",
      "loss:0.010751406662166119 \n",
      "accuracy:100.0\n",
      "..........\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:94001/100000 \n",
      "loss:0.023729274049401283 \n",
      "accuracy:95.83333134651184\n",
      "..........\n",
      "iteration:95001/100000 \n",
      "loss:0.003417074680328369 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:96001/100000 \n",
      "loss:0.01309977937489748 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:97001/100000 \n",
      "loss:0.011841499246656895 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:98001/100000 \n",
      "loss:0.001950435689650476 \n",
      "accuracy:100.0\n",
      "..........\n",
      "iteration:99001/100000 \n",
      "loss:0.06551812589168549 \n",
      "accuracy:100.0\n",
      "..........\n"
     ]
    }
   ],
   "source": [
    "# 定义正确的预测函数和正确率评估参数\n",
    "correctPred = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "# 最后将标准的交叉熵损失函数定义为损失值，这里是以adam为优化函数\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "tf.summary.scalar('loss',loss)\n",
    "tf.summary.scalar('Accrar',accuracy)\n",
    "merged=tf.summary.merge_all()\n",
    "logdir='tensorboard/'+ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"/\"\n",
    "writer=tf.summary.FileWriter(logdir,sess.graph)\n",
    "\n",
    "\n",
    "iterations = 100000\n",
    "for i in range(iterations):\n",
    "    # 下个批次的数据\n",
    "    next_batch, next_batch_labels = get_train_batch()\n",
    "    sess.run(optimizer,{input_data: next_batch, labels: next_batch_labels}) \n",
    "\n",
    "    # 每50次写入一次leadboard\n",
    "    if(i%50==0):\n",
    "        summary=sess.run(merged,{input_data: next_batch, labels: next_batch_labels})\n",
    "        writer.add_summary(summary,i)\n",
    "\n",
    "    if (i%1000==0):\n",
    "        loss_ = sess.run(loss, {input_data: next_batch, labels: next_batch_labels})\n",
    "\n",
    "        accuracy_=(sess.run(accuracy, {input_data: next_batch, labels: next_batch_labels})) * 100\n",
    "        print(\"iteration:{}/{}\".format(i+1, iterations),\n",
    "                  \"\\nloss:{}\".format(loss_),\n",
    "                  \"\\naccuracy:{}\".format(accuracy_))    \n",
    "        print('..........')  \n",
    "    # 每10000次保存一下模型\n",
    "    if(i%10000==0 and i!=0):\n",
    "        save_path=saver.save(sess,\"/Users/zouhao/Desktop/LSTM情感分析Test4/models/pretrained_lstm.ckpt\",global_step=i)\n",
    "        print(\"saved to %s\"% save_path)\n",
    "\n",
    "writer.close()#11:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11:20 11:44 4:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用模型进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/Users/zouhao/Desktop/0_10.txt\"\n",
    "with open(path,'r') as f:\n",
    "    line = f.readline().strip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great, and I suggest that you go see it before you judge.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/zouhao/Desktop/LSTM情感分析Test4/models/pretrained_lstm.ckpt-90000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"The name 'w1:0' refers to a Tensor which does not exist. The operation, 'w1', does not exist in the graph.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-159f554f5ea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"w1:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"w2:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m13.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m17.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_tensor_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3513\u001b[0m       raise TypeError(\"Tensor names are strings (or similar), not %s.\" %\n\u001b[1;32m   3514\u001b[0m                       type(name).__name__)\n\u001b[0;32m-> 3515\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3517\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_tensor_by_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3338\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3339\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3341\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3379\u001b[0m           raise KeyError(\"The name %s refers to a Tensor which does not \"\n\u001b[1;32m   3380\u001b[0m                          \u001b[0;34m\"exist. The operation, %s, does not exist in the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3381\u001b[0;31m                          \"graph.\" % (repr(name), repr(op_name)))\n\u001b[0m\u001b[1;32m   3382\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3383\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"The name 'w1:0' refers to a Tensor which does not exist. The operation, 'w1', does not exist in the graph.\""
     ]
    }
   ],
   "source": [
    "sess=tf.Session()    \n",
    "saver = tf.train.import_meta_graph('/Users/zouhao/Desktop/LSTM情感分析Test4/models/pretrained_lstm.ckpt-90000.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('/Users/zouhao/Desktop/LSTM情感分析Test4/models/'))\n",
    " \n",
    "graph = tf.get_default_graph()\n",
    "w1 = graph.get_tensor_by_name(\"w1:0\")\n",
    "w2 = graph.get_tensor_by_name(\"w2:0\")\n",
    "feed_dict ={w1:13.0,w2:17.0}\n",
    " \n",
    "op_to_restore = graph.get_tensor_by_name(\"op_to_restore:0\")\n",
    " \n",
    "print(sess.run(op_to_restore,feed_dict))\n",
    "sess.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
